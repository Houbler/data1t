##Итоговая аттестация

#Общее задание:
Общие требования и рекомендации для всех проектов
В качестве результата вашей работы по проекту вам необходимо предоставить репозиторий с целым(!) проектом. Выберите один из представленных проектов.
У проекта должна быть понятная структура, код и данные не должны лежать в одном месте.
К проекту обязательно должен быть приложен README-файл с подробным описанием проекта, его содержанием, использованным стеком, последовательностью шагов реализации проекта и блок-схемой проекта (опционально, но для наглядности лучше сделать).
В коде должны быть использованы правильные имена переменных, таблиц, функций. Имена переменных формата x1, a, AAA будут считаться ошибкой.
Желательно использовать комментарии в коде.
Выбор стека технологий является для вас ключевой задачей. Здесь вы свободны выбирать, но в итоговой презентации (об этом ниже) вам необходимо будет обосновать выбор того или иного инструмента для решения конкретной задачи.
В проектах с двумя режимами загрузки должна присутствовать оркестрация. Выбор инструмента оркестрации также остается за вами. Crontab тоже засчитывается.
В каждом из проектов, особенно в анализе логов, должен быть процесс data quality, по итогам которого данные будут проанализированы на корректность, исправлены все ошибки/опечатки, структура и типы данных приведены в необходимый формат. Также необходимо подготовить мини-отчет по качеству входных данных, если источников несколько, то для каждого из источников свой отчет.
В каждом проекте должна присутствовать ER-диаграмма вашей модели данных. Или, если вы решите все данные держать в одной плоской табличке (это не значит, что так делать правильно), то необходимо описание полей таблицы и их типов. Инструменты для отрисовки ER-диаграмм:
Gliffy, Draw.io, Miro, PlantUML.
Или любой другой на ваш выбор 
Вам необходимо подготовить итоговую презентацию, в которой необходимо отразить:
Название и общее описание проекта
Цели проекта с описание бизнес-задачи и требованиями
План реализации
Используемые технологии с обоснованием
Схемы/архитектуры с обоснованием 
Результаты разработки
Выводы

#Пятый проект:
Служба такси.
Все операции должны считаться локально.
Есть таблица, состоящая из поездок такси в Нью-Йорке.
Поле					Описание
VendorId				ИД компании
Trep_pickup_datetime	Время и дата, когда пассажир сел в такси
Trep_dropoff_datetime	Время и дата, когда пассажир вышел из такси
Passanger_count			Количество пассажиров
Trip_distance			Пройденное расстояние
Ratecodeid				Код скорости
Store_and_fwd_flag		Флаг, отвечающий за сохранение записи поездки перед ее отправкой поставщику
PulocationId			Широта, где была начата поездка
Dolocationid			Долгота, где была начата поездка
Payment_type			Тип оплаты
Fare_amount				Стоимость поездки
Mta_tax					Комиссия автопарка
Tip_amount				Чаевые
Tools_amount			Оплата за платные дороги
Improvement_surchange	Доплата за страховку
Total_amount			Полная стоимость поездки
Congestion_surchange	Дополнительный сбор 

Необходимо, используя таблицу поездок для каждого дня, рассчитать процент поездок по количеству человек в машине (без пассажиров, 1, 2, 3, 4 и более пассажиров). По итогу должна получиться таблица (parquet) с колонками date, percentage_zero, percentage_1p, percentage_2p, percentage_3p, percentage_4p_plus. Технологический стек — sql, scala (что-то одно). 
Также добавить столбцы к предыдущим результатам с самой дорогой и самой дешевой поездкой для каждой группы.

Дополнительно: также провести аналитику и построить график на тему «Как пройденное расстояние и количество пассажиров влияет на чаевые» в любом удобном инструменте.


#Исходные данные:
https://disk.yandex.ru/d/DKeoopbGH1Ttuw

Обработанные:
https://disk.yandex.ru/d/HLV1CE8vLmCD-A

*Они не поместились в github, но должны лежать в папке raw_data.*
*А еще я не могу пока выложить работу в github.*

Комментарий:
Для работы использовались следующие инструменты:
 - СУБД PostgreSQL, в котором разворачивается база, состоящая из core и data mart слоев.
	[DDL](DDL.sql)
 - Python и различные библиотеки использовающиеся в Jupyter Notebook: pandas, numpy, psycopg2, matplotlib, seaborn для считывания данных, их частичной обработки, построения графиков, вывода формата parquet, общения с БД.
	[Jupyter Notebook](scripts.ipynb)
 - Docker для поднятия postgres. 
	[Docker](docker-compose.yml)
 
В папке raw_data хранятся файл с изначальными данными, а также обработанный с помощью pandas для занесения в psql.
В саму БД загружаются только данные необходимые для анализа очищенные от пустых ячеек и ненужных столбцов.

Результат анализа занесен в папку out: mart.parquet для первого задания и output.png для второго.

Данные для входа в бд postgres:
 Database - db_itog
 User - user
 Password - secret
 Port - 5432